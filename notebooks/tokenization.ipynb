{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bbae85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, AutoModelForCausalLM, AutoTokenizer\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6fb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dfa5c6c07f41d7a183775843d97760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(base_model, padding_side=\"left\")\n",
    "llama = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# alpaca = PeftModel.from_pretrained(\n",
    "#     llama,\n",
    "#     \"tloen/alpaca-lora-7b\",\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "# unwind broken decapoda-research config\n",
    "llama.config.pad_token_id = llama_tokenizer.pad_token_id = 0  # unk\n",
    "llama.config.bos_token_id = 1\n",
    "llama.config.eos_token_id = 2\n",
    "\n",
    "llama.eval()\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    llama = torch.compile(llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ca7f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c85ad16fee415fbeaa1a3a06da3ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-6.7b\",\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "opt_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\", padding_side=\"left\")\n",
    "\n",
    "opt.eval()\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    opt = torch.compile(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b01ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Tell me about alpacas\", \"I really like cheese. How do I make a sandwich?\"]\n",
    "control = \"Tell me about alpacas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b7254018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_bos(a: torch.Tensor)->torch.Tensor:\n",
    "    a[:, -1] = 0\n",
    "    return torch.roll(a, shifts=1, dims=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "92c6ffb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0, 24948,   592,\n",
       "          1048,   394, 29886,   562,   294],\n",
       "        [    0,   306,  2289,   763,   923,   968, 29889,  1128,   437,   306,\n",
       "          1207,   263, 11982, 16416, 29973]], device='cuda:1'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokens = llama_tokenizer(prompts, padding=True, return_tensors=\"pt\").to('cuda')\n",
    "llama_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa977752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk><unk><unk><unk><unk><unk><unk><unk>Tell me about alpacas',\n",
       " '<unk>I really like cheese. How do I make a sandwich?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.batch_decode(llama_tokens.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f35c387e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>Tell me about alpacas']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.batch_decode(llama_control.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "926d2417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>Tell me about alpacas:222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = llama.generate(\n",
    "        **llama_control,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(llama_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "921f6c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>Tell me about alpacas.\\nA: Alpacas are members of the camelid family, which includes llamas, camels, and vicuñas. They are native to the Andes Mountains of South America, where they have lived for thousands of years. Alpacas are smaller than llamas, and they have a much finer, lighter fleece. Alpacas are raised for their fiber, which is used to make clothing, blankets, and other text']\n"
     ]
    }
   ],
   "source": [
    "new_mask = torch.clone(llama_control.attention_mask)\n",
    "new_mask[0,0] = 0\n",
    "with torch.no_grad():\n",
    "    outputs = llama.generate(\n",
    "        input_ids=llama_control.input_ids,\n",
    "        attention_mask=masked_bos(llama_control.attention_mask),\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(llama_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "37b9e149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>Tell me about alpacas.\\nA: Alpacas are members of the camelid family, which includes llamas, camels, and vicuñas. They are native to the Andes Mountains of South America, where they have lived for thousands of years. Alpacas are smaller than llamas, and they have a much finer, lighter fleece. Alpacas are raised for their fiber, which is used to make clothing, blankets, and other text']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = llama.generate(\n",
    "        input_ids=llama_control.input_ids,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(llama_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "221b90f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk><unk><unk><unk><unk><unk><unk><unk>Tell me about alpacas.\\nTell me about alpacas.\\nAlpacas are members of the camelid family, which includes llamas, vicunas, guanacos, and camels. Alpacas are native to the Andes Mountains of Peru, Chile, and Bolivia. They are smaller than llamas, and have a more delicate appearance. Alpacas are raised for their fiber, which is used to make clothing, blankets, and', \"<unk>I really like cheese. How do I make a sandwich?\\nI'm going to make a sandwich.\\nI'm going to make a sandwich. I'm going to make a sandwich. I'm going to make a sandwich. I'm going to make a sandwich. I'm going to make a sandwich. I'm going to make a sandwich. I'm going to make a sandwich. I'm going to make a sandwich. I'm going to make a sand\"]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = llama.generate(\n",
    "        **llama_tokens,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(llama_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2963067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk><unk><unk><unk><unk><unk><unk><unk>Tell me about alpacas.\\nI’m a big fan of alpacas. They’re very gentle, they’re very sweet, they’re very intelligent, and they’re very social. They’re very similar to llamas, but they’re a little bit smaller. They’re very similar to sheep, but they’re a little bit smaller. They’re very similar to goats, but they’re a little bit smaller. They’re very similar to c', '<unk>I really like cheese. How do I make a sandwich?\\nI\\'m not sure what you mean by \"make a sandwich\". You can make a sandwich with any kind of bread, cheese, and meat.\\nWhat is the best cheese to use for a sandwich?\\nI like cheddar cheese.\\nWhat is the best cheese to use for a sandwich? I like cheddar cheese.\\nWhat is the best cheese to use for a sandwich?\\nI like cheddar che']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = llama.generate(\n",
    "        input_ids=llama_tokens.input_ids,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(llama_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "277a7357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     1,     1,     1,     1,     1,     2, 35438,   162,    59,\n",
       "          1076, 21091,   281],\n",
       "        [    2,   100,   269,   101,  7134,     4,  1336,   109,    38,   146,\n",
       "            10, 15649,   116]], device='cuda:1'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_tokens = opt_tokenizer(prompts, padding=True, return_tensors=\"pt\").to('cuda')\n",
    "opt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "463178f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><pad><pad><pad><pad><pad></s>Tell me about alpacas',\n",
       " '</s>I really like cheese. How do I make a sandwich?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_tokenizer.batch_decode(opt_tokens.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb95725d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 35438,   162,    59,  1076, 21091,   281]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:1')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_control = opt_tokenizer(control, padding=True, return_tensors=\"pt\").to('cuda')\n",
    "opt_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "184b68e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>Tell me about alpacas']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_tokenizer.batch_decode(opt_control.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c74c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"</s>Tell me about alpacas.\\nThey're like llamas, but with more hair.\\nAnd they're more cuddly.</s>\"]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = opt.generate(\n",
    "        **opt_control,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(opt_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4bc843c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"</s>Tell me about alpacas.\\nThey're like llamas, but with more hair.\\nAnd they're more cuddly.</s>\"]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = opt.generate(\n",
    "        input_ids=opt_control.input_ids,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(opt_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a5671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"</s>Tell me about alpacas.\\nThey're like llamas, but they're smaller.\\nThey're very gentle.\\nThey're very smart.\\nThey're very funny.\\nThey're very, very funny.\\nThey're very, very smart.\\nThey're very, very gentle.\\nThey're very, very funny.\\nThey're very, very smart.\\nThey're very, very gentle.\\nThey're very, very funny.\\nThey're very, very smart.\\nThey're very,\"]\n"
     ]
    }
   ],
   "source": [
    "new_mask = torch.clone(opt_control.attention_mask)\n",
    "new_mask[0,0] = 0\n",
    "with torch.no_grad():\n",
    "    outputs = opt.generate(\n",
    "        input_ids=opt_control.input_ids,\n",
    "        attention_mask=new_mask,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(opt_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "704974f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<pad><pad><pad><pad><pad><pad></s>Tell me about alpacas.\\nThey're like llamas but with more hair.\\nAnd they're more cuddly.</s>\", '</s>I really like cheese. How do I make a sandwich?\\nI like cheese too. I like cheese on my sandwich.</s><pad><pad><pad><pad><pad><pad><pad><pad>']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = opt.generate(\n",
    "        **opt_tokens,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(opt_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0b13350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<pad><pad><pad><pad><pad><pad></s>Tell me about alpacas.\\nThey're like llamas but with more hair.\\nAnd they're more cuddly.</s>\", '</s>I really like cheese. How do I make a sandwich?\\nI like cheese too. I like cheese on my sandwich.</s><pad><pad><pad><pad><pad><pad><pad><pad>']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = opt.generate(\n",
    "        input_ids=opt_tokens.input_ids,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(opt_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "615bb5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.582530975341797"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.get_memory_footprint() / ((2**10)**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5d1f051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.402374267578125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.get_memory_footprint() / ((2**10)**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d963ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = PeftModel.from_pretrained(\n",
    "    llama,\n",
    "    \"tloen/alpaca-lora-7b\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d2697953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], device='cuda:1')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokens.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e7405512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_bos(llama_tokens.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5c0690dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk><unk><unk><unk><unk><unk><unk><unk>Tell me about alpacas.\\nA: Alpacas are members of the camelid family, which includes llamas, camels, and vicuñas. They are native to the Andes Mountains of South America, where they have lived for thousands of years. Alpacas are smaller than llamas, and they have a much finer, softer fleece. Alpacas are raised for their fiber, which is used to make clothing, blankets, and other text', '<unk>I really like cheese. How do I make a sandwich?\\nI like cheese. I like bread. I like ham. I like lettuce. I like tomatoes. I like mustard. I like mayonnaise. I like pickles. I like ketchup. I like peppers. I like onions. I like olives. I like jalapenos. I like cucumbers. I like bell peppers. I like mushrooms. I like avocados. I like bananas. I']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = alpaca.generate(\n",
    "        input_ids=llama_tokens.input_ids,\n",
    "        attention_mask=masked_bos(llama_tokens.attention_mask),\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(llama_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2356c735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk><unk><unk><unk><unk><unk><unk><unk>Tell me about alpacas.\\nA: Alpacas are members of the camelid family, which includes llamas, camels, and vicuñas. They are native to the Andes Mountains of South America, where they have lived for thousands of years. Alpacas are smaller than llamas, and they have a much finer, softer fleece. Alpacas are raised for their fiber, which is used to make clothing, blankets, and other text', '<unk>I really like cheese. How do I make a sandwich?\\nI like cheese. I like bread. I like ham. I like lettuce. I like tomatoes. I like mustard. I like mayonnaise. I like pickles. I like ketchup. I like peppers. I like onions. I like olives. I like jalapenos. I like cucumbers. I like bell peppers. I like mushrooms. I like avocados. I like bananas. I']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = alpaca.generate(\n",
    "        input_ids=llama_tokens.input_ids,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(llama_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "02152d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolq =  \"Ecuador at the FIFA World Cup -- The Ecuadorian national football team has appeared at three FIFA World Cups, the world's premier football tournament for national football teams. Ecuador's first participation in the World Cup was in 2002. Their best performance was in 2006, where they were eliminated in the Round of 16.\\nquestion: has ecuador ever been in the world cup\\nanswer: yes\\n\\nEcuador at the FIFA World Cup -- The Ecuadorian national football team has appeared at three FIFA World Cups, the world's premier football tournament for national football teams. Ecuador's first participation in the World Cup was in 2002. Their best performance was in 2006, where they were eliminated in the Round of 16.\\nquestion: has ecuador ever been in the world cup\\nanswer: yes\\n\\nEnterprise value -- Enterprise value (EV), total enterprise value (TEV), or firm value (FV) is an economic measure reflecting the market value of a business. It is a sum of claims by all claimants: creditors (secured and unsecured) and shareholders (preferred and common). Enterprise value is one of the fundamental metrics used in business valuation, financial modeling, accounting, portfolio analysis, and risk analysis.\\nquestion: is enterprise value the same as firm value\\nanswer\"\n",
    "boolq_tokens = llama_tokenizer(boolq, padding=True, return_tensors='pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d6e90666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<unk>Ecuador at the FIFA World Cup -- The Ecuadorian national football team has appeared at three FIFA World Cups, the world's premier football tournament for national football teams. Ecuador's first participation in the World Cup was in 2002. Their best performance was in 2006, where they were eliminated in the Round of 16.\\nquestion: has ecuador ever been in the world cup\\nanswer: yes\\n\\nEcuador at the FIFA World Cup -- The Ecuadorian national football team has appeared at three FIFA World Cups, the world's premier football tournament for national football teams. Ecuador's first participation in the World Cup was in 2002. Their best performance was in 2006, where they were eliminated in the Round of 16.\\nquestion: has ecuador ever been in the world cup\\nanswer: yes\\n\\nEnterprise value -- Enterprise value (EV), total enterprise value (TEV), or firm value (FV) is an economic measure reflecting the market value of a business. It is a sum of claims by all claimants: creditors (secured and unsecured) and shareholders (preferred and common). Enterprise value is one of the fundamental metrics used in business valuation, financial modeling, accounting, portfolio analysis, and risk analysis.\\nquestion: is enterprise value the same as firm value\\nanswer: no\\n\\nFirm value -- Firm value is the total value of a company's assets, including its intangible assets such as its brand name, customer relationships, and intellectual property. It is the sum of the market value of the company's equity and the book value of its debt. Firm value is one of the fundamental metrics used in business valuation, financial modeling, accounting, portfolio analysis, and risk analysis.\\nquestion: is firm\"]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = llama.generate(\n",
    "        **boolq_tokens,\n",
    "        temperature=0,\n",
    "        max_new_tokens=100,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True)\n",
    "print(llama_tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "88c7e2b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[200], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgen_len\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_len' is not defined"
     ]
    }
   ],
   "source": [
    "gen_len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7b7c910d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.078125"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.scores[0].squeeze().log_softmax(-1)[outputs.sequences[:, -100+1].item()].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6398923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no']\n"
     ]
    }
   ],
   "source": [
    "print(llama_tokenizer.batch_decode(outputs.sequences[:, -100+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cf64fd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([694], device='cuda:1')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences[:, -100+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "720e9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(model, tokenizer, prompts, batch_size, mask_bos):\n",
    "    output_tokens = torch.empty(0, dtype=torch.int64).to('cuda:0')\n",
    "\n",
    "    num_batches = ceil(len(prompts) / batch_size)\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        start = batch * batch_size\n",
    "        end = min((batch + 1) * batch_size, len(prompts))\n",
    "\n",
    "    gen_len=5\n",
    "    # tokenize by batch to mitigate effect of long outliers\n",
    "    tokens = tokenizer(prompts[start:end], padding=True, return_tensors=\"pt\").to('cuda:0')\n",
    "    attention_mask = masked_bos(tokens.attention_mask) if mask_bos else tokens.attention_mask\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=tokens.input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=gen_len,\n",
    "            temperature=0,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "    output_tokens = torch.cat((output_tokens, outputs.sequences[:, -gen_len:]))\n",
    "\n",
    "    output_text = tokenizer.batch_decode(output_tokens)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "adf2dd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[': no\\n\\nEnter', ': no\\n\\nEnter']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inference(llama, llama_tokenizer, [boolq]*2, 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672697d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
